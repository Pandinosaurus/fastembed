{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7412714c",
   "metadata": {},
   "source": [
    "# Porting to ONNX\n",
    "\n",
    "This notebook demonstrates how to port models from Transformers/PyTorch package to ONNX. It is based on the [Optimum](https://github.com/huggingface/optimum) library.\n",
    "\n",
    "## Installation\n",
    "\n",
    "We use [poetry](https://python-poetry.org/docs/cli) to manage dependencies. To install the dependencies, run:\n",
    "\n",
    "```bash\n",
    "poetry install\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e9dbcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c37e1fda-c7f1-46e7-a5d4-19fa05c36ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "from optimum.onnxruntime import AutoOptimizationConfig, ORTModelForFeatureExtraction, ORTOptimizer\n",
    "from optimum.pipelines import pipeline\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1ecf0b6-db81-4da3-b47f-e31460ccfbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and PyTorch model from HuggingFace Transformers\n",
    "model_id = \"BAAI/bge-small-en-v1.5\"\n",
    "hf_model = AutoModel.from_pretrained(model_id)\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "quantize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a38f5aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input texts can be in any language, not just English.\n",
    "# Each input text should start with \"query: \" or \"passage: \", even for non-English texts.\n",
    "# For tasks other than retrieval, you can simply use the \"query: \" prefix.\n",
    "multilingual_queries = [\n",
    "    \"query: how much protein should a female eat\",\n",
    "    \"query: 南瓜的家常做法\",\n",
    "    \"query: भारत का राष्ट्रीय खेल कौन-सा है?\",  # Hindi text\n",
    "    \"query: భారత్ దేశంలో రాష్ట్రపతి ఎవరు?\",  # Telugu text\n",
    "    \"query: இந்தியாவின் தேசிய கோப்பை எது?\",  # Tamil text\n",
    "    \"query: ಭಾರತದಲ್ಲಿ ರಾಷ್ಟ್ರಪತಿ ಯಾರು?\",  # Kannada text\n",
    "    \"query: ഇന്ത്യയുടെ രാഷ്ട്രീയ ഗാനം എന്താണ്?\",  # Malayalam text\n",
    "]\n",
    "\n",
    "english_texts = [\n",
    "    \"India: Where the Taj Mahal meets spicy curry.\",\n",
    "    \"Machine Learning: Turning data into knowledge, one algorithm at a time.\",\n",
    "    \"Python: The language that makes programming a piece of cake.\",\n",
    "    \"fastembed: Accelerating embeddings for lightning-fast similarity search.\",\n",
    "    \"Qdrant: The ultimate tool for high-dimensional indexing and search.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f8c761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "def hf_embed(model_id: str, inputs: List[str]):\n",
    "    # Tokenize the input texts\n",
    "    batch_dict = hf_tokenizer(inputs, max_length=512, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = hf_model(**batch_dict)\n",
    "    embeddings = average_pool(outputs.last_hidden_state, batch_dict[\"attention_mask\"])\n",
    "\n",
    "    # normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    return embeddings.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69bb4501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05485763,  0.08136623, -0.00395789, ...,  0.02512371,\n",
       "        -0.03349504, -0.0593129 ],\n",
       "       [ 0.01078518,  0.01582215,  0.04614557, ..., -0.01674951,\n",
       "        -0.00244641, -0.06179965],\n",
       "       [-0.06607923, -0.01235531, -0.00689854, ...,  0.10634594,\n",
       "         0.12025263, -0.05135345],\n",
       "       [-0.07568254,  0.00908228, -0.02221818, ...,  0.00177038,\n",
       "        -0.0325426 ,  0.05233581],\n",
       "       [-0.07008213,  0.02070545,  0.02720274, ..., -0.01158645,\n",
       "        -0.01457597,  0.01262206]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_embed(inputs=english_texts, model_id=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9d1594",
   "metadata": {},
   "source": [
    "## Load the model using ORTModelForFeatureExtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "451dbd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export to ONNX.\n",
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "Using framework PyTorch: 2.1.2\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/fst/lib/python3.9/site-packages/optimum/onnxruntime/configuration.py:770: FutureWarning: disable_embed_layer_norm will be deprecated soon, use disable_embed_layer_norm_fusion instead, disable_embed_layer_norm_fusion is set to True.\n",
      "  warnings.warn(\n",
      "The argument use_external_data_format in the ORTOptimizer.optimize() method is deprecated and will be removed in optimum 2.0.\n",
      "Optimizing model...\n",
      "There is no gpu for onnxruntime to do optimization.\n",
      "Configuration saved in saved_models/ort_config.json\n",
      "Optimized model saved at: saved_models (external data format: False; saved all tensor to one file: True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('saved_models')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ORTModelForFeatureExtraction.from_pretrained(model_id, export=True)\n",
    "\n",
    "# Load the optimization configuration detailing the optimization we wish to apply\n",
    "if quantize:\n",
    "    optimization_config = AutoOptimizationConfig.O4()\n",
    "    optimization_config = AutoOptimizationConfig.O4()\n",
    "    optimizer = ORTOptimizer.from_pretrained(model)\n",
    "\n",
    "save_dir = Path(\"saved_models\") # Path to save the optimized model, working directory\n",
    "optimizer.optimize(save_dir=save_dir, optimization_config=optimization_config, use_external_data_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ea5bd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ORTModelForFeatureExtraction.from_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68d7ab8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402e707a28ea4b2baf6c7eea382933b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_optimized.onnx:   0%|          | 0.00/66.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.push_to_hub(save_directory=save_dir, repository_id=\"Qdrant/bge-small-en-v1.5-onnx-Q\", use_auth_token=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
