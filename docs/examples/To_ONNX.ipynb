{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7412714c",
   "metadata": {},
   "source": [
    "# Porting to ONNX\n",
    "\n",
    "This notebook demonstrates how to port models from Transformers/PyTorch package to ONNX. It is based on the [Optimum](https://github.com/huggingface/optimum) library.\n",
    "\n",
    "## Installation\n",
    "\n",
    "We use [poetry](https://python-poetry.org/docs/cli) to manage dependencies. To install the dependencies, run:\n",
    "\n",
    "```bash\n",
    "poetry install\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e9dbcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c37e1fda-c7f1-46e7-a5d4-19fa05c36ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "from optimum.onnxruntime import AutoOptimizationConfig, ORTModelForFeatureExtraction, ORTOptimizer, ORTModel\n",
    "from optimum.pipelines import pipeline\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ecf0b6-db81-4da3-b47f-e31460ccfbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and PyTorch model from HuggingFace Transformers\n",
    "model_id = \"BAAI/bge-small-en-v1.5\"\n",
    "hf_model = AutoModel.from_pretrained(model_id)\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "quantize = True\n",
    "if quantize:\n",
    "    repository_id = f\"Qdrant/{model_id.split('/')[1]}-onnx-Q\"\n",
    "else:\n",
    "    repository_id = f\"Qdrant/{model_id.split('/')[1]}-onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a38f5aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input texts can be in any language, not just English.\n",
    "# Each input text should start with \"query: \" or \"passage: \", even for non-English texts.\n",
    "# For tasks other than retrieval, you can simply use the \"query: \" prefix.\n",
    "multilingual_queries = [\n",
    "    \"query: how much protein should a female eat\",\n",
    "    \"query: 南瓜的家常做法\",\n",
    "    \"query: भारत का राष्ट्रीय खेल कौन-सा है?\",  # Hindi text\n",
    "    \"query: భారత్ దేశంలో రాష్ట్రపతి ఎవరు?\",  # Telugu text\n",
    "    \"query: இந்தியாவின் தேசிய கோப்பை எது?\",  # Tamil text\n",
    "    \"query: ಭಾರತದಲ್ಲಿ ರಾಷ್ಟ್ರಪತಿ ಯಾರು?\",  # Kannada text\n",
    "    \"query: ഇന്ത്യയുടെ രാഷ്ട്രീയ ഗാനം എന്താണ്?\",  # Malayalam text\n",
    "]\n",
    "\n",
    "english_texts = [\n",
    "    \"India: Where the Taj Mahal meets spicy curry.\",\n",
    "    \"Machine Learning: Turning data into knowledge, one algorithm at a time.\",\n",
    "    \"Python: The language that makes programming a piece of cake.\",\n",
    "    \"fastembed: Accelerating embeddings for lightning-fast similarity search.\",\n",
    "    \"Qdrant: The ultimate tool for high-dimensional indexing and search.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f8c761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "def hf_embed(model_id: str, inputs: List[str]):\n",
    "    # Tokenize the input texts\n",
    "    batch_dict = hf_tokenizer(inputs, max_length=512, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = hf_model(**batch_dict)\n",
    "    embeddings = average_pool(outputs.last_hidden_state, batch_dict[\"attention_mask\"])\n",
    "\n",
    "    # normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    return embeddings.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69bb4501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05485763,  0.08136623, -0.00395789, ...,  0.02512371,\n",
       "        -0.03349504, -0.0593129 ],\n",
       "       [ 0.01078518,  0.01582215,  0.04614557, ..., -0.01674951,\n",
       "        -0.00244641, -0.06179965],\n",
       "       [-0.06607923, -0.01235531, -0.00689854, ...,  0.10634594,\n",
       "         0.12025263, -0.05135345],\n",
       "       [-0.07568254,  0.00908228, -0.02221818, ...,  0.00177038,\n",
       "        -0.0325426 ,  0.05233581],\n",
       "       [-0.07008213,  0.02070545,  0.02720274, ..., -0.01158645,\n",
       "        -0.01457597,  0.01262206]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_embed(inputs=english_texts, model_id=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9d1594",
   "metadata": {},
   "source": [
    "## Load the model using ORTModelForFeatureExtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "451dbd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export to ONNX.\n",
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "Using framework PyTorch: 2.1.2\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/fst/lib/python3.11/site-packages/optimum/onnxruntime/configuration.py:770: FutureWarning: disable_embed_layer_norm will be deprecated soon, use disable_embed_layer_norm_fusion instead, disable_embed_layer_norm_fusion is set to True.\n",
      "  warnings.warn(\n",
      "Optimizing model...\n",
      "Configuration saved in local_cache/ort_config.json\n",
      "Optimized model saved at: local_cache (external data format: False; saved all tensor to one file: True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('local_cache')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ORTModelForFeatureExtraction.from_pretrained(model_id, export=True)\n",
    "\n",
    "# Load the optimization configuration detailing the optimization we wish to apply\n",
    "if quantize:\n",
    "    optimization_config = AutoOptimizationConfig.O3()\n",
    "    optimizer = ORTOptimizer.from_pretrained(model)\n",
    "\n",
    "save_dir = Path(\"local_cache\") # Path to save the optimized model, working directory\n",
    "optimizer.optimize(save_dir=save_dir, optimization_config=optimization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ea5bd5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not create tensor from given input list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5,) + inhomogeneous part.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(repository_id)\n\u001b[1;32m      4\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer(english_texts)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/fst/lib/python3.11/site-packages/optimum/modeling_base.py:90\u001b[0m, in \u001b[0;36mOptimizedModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/fst/lib/python3.11/site-packages/optimum/onnxruntime/modeling_ort.py:958\u001b[0m, in \u001b[0;36mORTModelForFeatureExtraction.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, **kwargs)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    956\u001b[0m     onnx_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token_type_ids\n\u001b[0;32m--> 958\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monnx_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_names[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_torch:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/fst/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:220\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    218\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not create tensor from given input list"
     ]
    }
   ],
   "source": [
    "model = ORTModelForFeatureExtraction.from_pretrained(save_dir)\n",
    "# optimized_model = AutoModel.from_pretrained(repository_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(repository_id)\n",
    "tokens = tokenizer(english_texts)\n",
    "model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d7ab8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# model.push_to_hub(save_directory=save_dir, repository_id=repository_id, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1da6a780",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not create tensor from given input list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5,) + inhomogeneous part.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(repository_id)\n\u001b[1;32m      3\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer(english_texts)\n\u001b[0;32m----> 4\u001b[0m \u001b[43moptimized_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/fst/lib/python3.11/site-packages/optimum/modeling_base.py:90\u001b[0m, in \u001b[0;36mOptimizedModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/fst/lib/python3.11/site-packages/optimum/onnxruntime/modeling_ort.py:958\u001b[0m, in \u001b[0;36mORTModelForFeatureExtraction.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, **kwargs)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    956\u001b[0m     onnx_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token_type_ids\n\u001b[0;32m--> 958\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monnx_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_names[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_torch:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/fst/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:220\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    218\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not create tensor from given input list"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
